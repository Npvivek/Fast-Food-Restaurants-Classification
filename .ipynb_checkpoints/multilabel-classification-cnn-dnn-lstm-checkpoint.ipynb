{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0095a502",
   "metadata": {
    "papermill": {
     "duration": 0.0258,
     "end_time": "2024-11-01T03:58:22.976968",
     "exception": false,
     "start_time": "2024-11-01T03:58:22.951168",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e16d04",
   "metadata": {
    "papermill": {
     "duration": 2.231756,
     "end_time": "2024-11-01T03:58:25.234775",
     "exception": false,
     "start_time": "2024-11-01T03:58:23.003019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from geopy.geocoders import Nominatim\n",
    "import folium\n",
    "import plotly.graph_objs as go\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalMaxPool1D, Dropout, Conv1D, Activation, Input, LSTM\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d776f",
   "metadata": {
    "papermill": {
     "duration": 0.027177,
     "end_time": "2024-11-01T03:58:25.288429",
     "exception": false,
     "start_time": "2024-11-01T03:58:25.261252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Load The Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dac2ecc",
   "metadata": {
    "papermill": {
     "duration": 0.266593,
     "end_time": "2024-11-01T03:58:25.581850",
     "exception": false,
     "start_time": "2024-11-01T03:58:25.315257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Datafiniti_Fast_Food_Restaurants_May19.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833ce1fa",
   "metadata": {
    "papermill": {
     "duration": 0.025721,
     "end_time": "2024-11-01T03:58:25.633912",
     "exception": false,
     "start_time": "2024-11-01T03:58:25.608191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Looking Into Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e71b3",
   "metadata": {
    "papermill": {
     "duration": 0.07081,
     "end_time": "2024-11-01T03:58:25.730775",
     "exception": false,
     "start_time": "2024-11-01T03:58:25.659965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#lets take a look \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1b212e-52a7-4e63-884f-a8dd9fb13db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd4088a",
   "metadata": {
    "papermill": {
     "duration": 0.064716,
     "end_time": "2024-11-01T03:58:25.823458",
     "exception": false,
     "start_time": "2024-11-01T03:58:25.758742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#checking dfthe info \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b32e72",
   "metadata": {
    "papermill": {
     "duration": 0.050622,
     "end_time": "2024-11-01T03:58:25.901753",
     "exception": false,
     "start_time": "2024-11-01T03:58:25.851131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#checking null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f39cdb3-af58-4eee-bda6-2098a57e5ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values in 'websites' column\n",
    "df['websites'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e162f035",
   "metadata": {
    "papermill": {
     "duration": 0.026722,
     "end_time": "2024-11-01T03:58:25.955588",
     "exception": false,
     "start_time": "2024-11-01T03:58:25.928866",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "there is only null values in website variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a9f667",
   "metadata": {
    "papermill": {
     "duration": 0.29089,
     "end_time": "2024-11-01T03:58:26.523664",
     "exception": false,
     "start_time": "2024-11-01T03:58:26.232774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#looking into city variable\n",
    "df['city'].value_counts()[:20].plot(kind= 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f69745",
   "metadata": {
    "papermill": {
     "duration": 0.355867,
     "end_time": "2024-11-01T03:58:26.907450",
     "exception": false,
     "start_time": "2024-11-01T03:58:26.551583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['name'].value_counts()[:20].plot(kind= 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fbf38a",
   "metadata": {
    "papermill": {
     "duration": 0.029148,
     "end_time": "2024-11-01T03:58:26.965621",
     "exception": false,
     "start_time": "2024-11-01T03:58:26.936473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Data Cleaing**\n",
    "\n",
    "we are going to work on catageroy variable and in that variable there is some noise we have to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4cf2ea",
   "metadata": {
    "papermill": {
     "duration": 0.063358,
     "end_time": "2024-11-01T03:58:27.117175",
     "exception": false,
     "start_time": "2024-11-01T03:58:27.053817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['new_categories']=df['categories'].apply(lambda x:x.lower())\n",
    "\n",
    "df[['categories','new_categories']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82780280",
   "metadata": {
    "papermill": {
     "duration": 0.028451,
     "end_time": "2024-11-01T03:58:27.175595",
     "exception": false,
     "start_time": "2024-11-01T03:58:27.147144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Removing Stop Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db213378",
   "metadata": {
    "papermill": {
     "duration": 0.283126,
     "end_time": "2024-11-01T03:58:27.487680",
     "exception": false,
     "start_time": "2024-11-01T03:58:27.204554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42cf08",
   "metadata": {
    "papermill": {
     "duration": 0.373672,
     "end_time": "2024-11-01T03:58:27.890802",
     "exception": false,
     "start_time": "2024-11-01T03:58:27.517130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df['new_categories'] = df['categories'].str.lower().apply(\n",
    "    lambda x: \" \".join(word for word in x.split() if word not in stop_words)\n",
    ")\n",
    "\n",
    "df[['categories','new_categories']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9013b7fa",
   "metadata": {
    "papermill": {
     "duration": 0.029963,
     "end_time": "2024-11-01T03:58:27.950041",
     "exception": false,
     "start_time": "2024-11-01T03:58:27.920078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Removing unwanted text patterns from the categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa00b00d",
   "metadata": {
    "papermill": {
     "duration": 1.162022,
     "end_time": "2024-11-01T03:58:29.210031",
     "exception": false,
     "start_time": "2024-11-01T03:58:28.048009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove unwanted text patterns from 'new_categories' efficiently\n",
    "unwanted_terms = [\n",
    "    'restaurants', 'restaurant', 'take out', 'carry-out food', 'manufacturers',\n",
    "    'cypress station', 'caterers', 'delivery service', 'uncategorized', \n",
    "    \"women's clothing\", 'delis', 'hotels and motel', '&', 'catering', \n",
    "    'airport devonshire', 'airport', 'bars', '- full service', 'clubs', \n",
    "    'pubs', 'american canoga park', 'american cape fear', \n",
    "    'american downtown blacksburg'\n",
    "]\n",
    "\n",
    "for term in unwanted_terms:\n",
    "    df['new_categories'] = df['new_categories'].str.replace(term, '', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06638646",
   "metadata": {
    "papermill": {
     "duration": 0.049194,
     "end_time": "2024-11-01T03:58:29.290326",
     "exception": false,
     "start_time": "2024-11-01T03:58:29.241132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[['categories','new_categories']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f4069",
   "metadata": {
    "papermill": {
     "duration": 0.029411,
     "end_time": "2024-11-01T03:58:29.349777",
     "exception": false,
     "start_time": "2024-11-01T03:58:29.320366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaddc9a9",
   "metadata": {
    "papermill": {
     "duration": 0.02967,
     "end_time": "2024-11-01T03:58:30.093139",
     "exception": false,
     "start_time": "2024-11-01T03:58:30.063469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **checking unique values from each colum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c67c2",
   "metadata": {
    "papermill": {
     "duration": 0.474702,
     "end_time": "2024-11-01T03:58:30.597734",
     "exception": false,
     "start_time": "2024-11-01T03:58:30.123032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a summary DataFrame for number of unique values in each column\n",
    "unique_df = pd.DataFrame()\n",
    "unique_df['Features'] = df.columns\n",
    "unique_df['Uniques'] = [df[col].nunique() for col in df.columns]\n",
    "\n",
    "# Plotting the unique values\n",
    "plt.figure(figsize=(15, 7))\n",
    "sns.barplot(x=unique_df['Features'], y=unique_df['Uniques'], alpha=1)\n",
    "plt.title('Bar Plot for Number of Unique Values in Each Column', weight='bold', size=15)\n",
    "plt.ylabel('#Unique values', size=12, weight='bold')\n",
    "plt.xlabel('Features', size=12, weight='bold')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d67557",
   "metadata": {
    "papermill": {
     "duration": 0.030123,
     "end_time": "2024-11-01T03:58:30.658280",
     "exception": false,
     "start_time": "2024-11-01T03:58:30.628157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Observations :\n",
    "1. we have data from only one country US and nearly about  from 2810 cities.\n",
    "2. we have data from 47 province."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab1eb6c",
   "metadata": {
    "papermill": {
     "duration": 0.031339,
     "end_time": "2024-11-01T03:58:30.720132",
     "exception": false,
     "start_time": "2024-11-01T03:58:30.688793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Understanding the common words used in the new_categories: WordCloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec984439",
   "metadata": {
    "papermill": {
     "duration": 1.580339,
     "end_time": "2024-11-01T03:58:32.330820",
     "exception": false,
     "start_time": "2024-11-01T03:58:30.750481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_words = ' '.join([text for text in df['new_categories']])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=110).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud of Categories\", fontsize=24)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb975e5",
   "metadata": {
    "papermill": {
     "duration": 0.055088,
     "end_time": "2024-11-01T03:58:32.423929",
     "exception": false,
     "start_time": "2024-11-01T03:58:32.368841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for the count of rest. area wise we groupby addres and count of rest. reat_name\n",
    "df1 = df.groupby(['province'])['name'].aggregate('count').reset_index().sort_values('name', ascending=False)\n",
    "df1.rename(columns = {'name':'Number_of_resto'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0c23b7",
   "metadata": {
    "papermill": {
     "duration": 0.054715,
     "end_time": "2024-11-01T03:58:32.516674",
     "exception": false,
     "start_time": "2024-11-01T03:58:32.461959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#info of df1\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843de9ec",
   "metadata": {
    "papermill": {
     "duration": 0.05356,
     "end_time": "2024-11-01T03:58:32.608178",
     "exception": false,
     "start_time": "2024-11-01T03:58:32.554618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#displaying the values in df1\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e04ffc",
   "metadata": {
    "papermill": {
     "duration": 0.491115,
     "end_time": "2024-11-01T03:58:33.137633",
     "exception": false,
     "start_time": "2024-11-01T03:58:32.646518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ploting the graph between restaurnt name and addres\n",
    "fig , ax = plt.subplots(figsize = (18,8))\n",
    "splot = sns.barplot(x = 'province', y = 'Number_of_resto' , data = df1.head(10) , ax=ax)\n",
    "\n",
    "# now simply assign the bar values to\n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center',\n",
    "                   va = 'center', xytext = (0, 9), textcoords = 'offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206a656d",
   "metadata": {
    "papermill": {
     "duration": 0.058704,
     "end_time": "2024-11-01T03:58:33.235477",
     "exception": false,
     "start_time": "2024-11-01T03:58:33.176773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for the count of rest. city wise we groupby city and count of rest. reat_name\n",
    "df1 = df.groupby(['city'])['name'].aggregate('count').reset_index().sort_values('name', ascending=False)\n",
    "df1.rename(columns = {'name':'Number_of_resto'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ab9fc",
   "metadata": {
    "papermill": {
     "duration": 0.370028,
     "end_time": "2024-11-01T03:58:33.644688",
     "exception": false,
     "start_time": "2024-11-01T03:58:33.274660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ploting the graph between restaurnt name and city\n",
    "fig , ax = plt.subplots(figsize = (18,8))\n",
    "splot = sns.barplot(x = 'city', y = 'Number_of_resto' , data = df1.head(10) , ax=ax)\n",
    "\n",
    "# now simply assign the bar values to\n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_height(), '.1f'), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center',\n",
    "                   va = 'center', xytext = (0, 9), textcoords = 'offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4413923",
   "metadata": {
    "papermill": {
     "duration": 0.040113,
     "end_time": "2024-11-01T03:58:33.724864",
     "exception": false,
     "start_time": "2024-11-01T03:58:33.684751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Plotting of the Restaurants on map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa2cd7f",
   "metadata": {
    "papermill": {
     "duration": 0.473298,
     "end_time": "2024-11-01T03:58:35.901255",
     "exception": false,
     "start_time": "2024-11-01T03:58:35.427957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "#!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "#!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "import folium # map rendering library\n",
    "\n",
    "import requests # library to handle requests\n",
    "from pandas import json_normalize  # new line\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d0ac6",
   "metadata": {
    "papermill": {
     "duration": 0.951009,
     "end_time": "2024-11-01T03:58:36.958311",
     "exception": false,
     "start_time": "2024-11-01T03:58:36.007302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "address = 'US'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"tr_explorer\", timeout=10)  # Set timeout to 10 seconds\n",
    "try:\n",
    "    location = geolocator.geocode(address)\n",
    "    if location:\n",
    "        latitude = location.latitude\n",
    "        longitude = location.longitude\n",
    "        print('The geographical coordinate of US are {}, {}.'.format(latitude, longitude))\n",
    "    else:\n",
    "        print('Location not found.')\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a980cad3",
   "metadata": {
    "papermill": {
     "duration": 16.017347,
     "end_time": "2024-11-01T03:58:53.085404",
     "exception": false,
     "start_time": "2024-11-01T03:58:37.068057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create map of Toronto using latitude and longitude values\n",
    "map_bng = folium.Map(location=[latitude, longitude], zoom_start=9)\n",
    "\n",
    "# add markers to map\n",
    "for latitude, longitude, address, city in zip(df['latitude'], df['longitude'], df['address'], df['city']):\n",
    "    label = '{}, {}'.format(address, city)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [latitude, longitude],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(map_bng)  \n",
    "    \n",
    "map_bng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041522ca",
   "metadata": {
    "papermill": {
     "duration": 0.462521,
     "end_time": "2024-11-01T03:58:57.264170",
     "exception": false,
     "start_time": "2024-11-01T03:58:56.801649",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86963245",
   "metadata": {
    "papermill": {
     "duration": 0.462104,
     "end_time": "2024-11-01T03:58:58.215646",
     "exception": false,
     "start_time": "2024-11-01T03:58:57.753542",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### combining colums to details address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e70de74",
   "metadata": {
    "papermill": {
     "duration": 0.503946,
     "end_time": "2024-11-01T03:58:59.180461",
     "exception": false,
     "start_time": "2024-11-01T03:58:58.676515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.fillna('', inplace=True)\n",
    "df[\"detailed_address\"] = df[\"name\"] + ' ' + df[\"address\"] + ' ' + df[\"city\"] + ' ' + df[\"country\"] + ' ' + df[\"latitude\"].astype(str) + ' ' + df[\"longitude\"].astype(str) + ' ' + df[\"postalCode\"] + ' ' + df['province'] + ' ' + df['websites']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06347c0c",
   "metadata": {
    "papermill": {
     "duration": 0.50197,
     "end_time": "2024-11-01T03:59:00.143836",
     "exception": false,
     "start_time": "2024-11-01T03:58:59.641866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = df[[\"detailed_address\",\"new_categories\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f1bce",
   "metadata": {
    "papermill": {
     "duration": 0.478497,
     "end_time": "2024-11-01T03:59:01.122539",
     "exception": false,
     "start_time": "2024-11-01T03:59:00.644042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating new variables as we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebd67a1",
   "metadata": {
    "papermill": {
     "duration": 4.926872,
     "end_time": "2024-11-01T03:59:06.510480",
     "exception": false,
     "start_time": "2024-11-01T03:59:01.583608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df = pd.concat([df1.drop('new_categories', axis=1), df['new_categories'].str.get_dummies(sep=\",\")], axis=1)\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d192b974",
   "metadata": {
    "papermill": {
     "duration": 0.490497,
     "end_time": "2024-11-01T03:59:07.462291",
     "exception": false,
     "start_time": "2024-11-01T03:59:06.971794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24de1071",
   "metadata": {
    "papermill": {
     "duration": 0.469097,
     "end_time": "2024-11-01T03:59:08.384599",
     "exception": false,
     "start_time": "2024-11-01T03:59:07.915502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79df8fe",
   "metadata": {
    "papermill": {
     "duration": 0.46666,
     "end_time": "2024-11-01T03:59:09.304629",
     "exception": false,
     "start_time": "2024-11-01T03:59:08.837969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#to see columns converting it into list\n",
    "columns = new_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a907d891",
   "metadata": {
    "papermill": {
     "duration": 0.460817,
     "end_time": "2024-11-01T03:59:11.167079",
     "exception": false,
     "start_time": "2024-11-01T03:59:10.706262",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "so munch noise . unwanted variables are present in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf667c4",
   "metadata": {
    "papermill": {
     "duration": 0.476291,
     "end_time": "2024-11-01T03:59:12.099422",
     "exception": false,
     "start_time": "2024-11-01T03:59:11.623131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''#printing all colm names \n",
    "for x in range(len(columns)):\n",
    "    print (columns[x])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d8ddcb",
   "metadata": {
    "papermill": {
     "duration": 0.523088,
     "end_time": "2024-11-01T03:59:13.081624",
     "exception": false,
     "start_time": "2024-11-01T03:59:12.558536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dropping unwanted variables\n",
    "new_df1 =new_df.drop([ 'lounge','mediterranean' , 'office' ,'australian','auto leasing','auto renting','auto repairing','automated teller machines (atm)',\n",
    "                      'autos','baby gear','baby products','bagels','bagels','bakeries',\"bakers' supplies\",'bakery','bakery' ,'bar supplies','barbecue grills supplies',\n",
    "                      'barboursville ','bartending service','bath products','beauty salons','bedding','beverage','beverages','beverages retail','big box store',\n",
    "                      'billiard parlors','billiard table ','biosil','birmingham ', 'bistro','bistro', 'bistros','black mountain ', 'bladensburg ', 'bliss',\n",
    "                      'blytheville ', 'bothell ', 'bowling','box lunches','braun','brew ', 'brewers','brookneal ', 'brooksville ', 'broomall ' ,'brunch' ,'buffet' \n",
    "                      ,'buffet'  ,'building contractors', 'business  personal coaches','business development','business schools','business services','cabinets',\n",
    "                      'cable internet','cable tv','caf','caf diplomat','cafes','cafeteria ', 'cafeterias','canby ','canonsburg ','cantonese ','carnivals','carpenters',\n",
    "                      'carry out','cary ','casino','cedar hill ','cedar rapids ','centre ','centreville ', 'chain','chain ','charleston ','charlotte ','cheap eats',\n",
    "                      'check cashing service','chelsea ','child care','childersburg ',\"children's clothing\",'clarisonic','clayton ','clermont ','clothing',\n",
    "                      'clothing alterations','cocktail bar','cocoa ','coffee brewing devices','coffee makers','coffee retail','coffee tea','coffeehouses',\n",
    "                      'collectibles','college academic building','college quad','columbus ','comfort food ','commercial photographers','commercial printing',\n",
    "                      'commercial refrigerators','communications','computer internet services','computer online services','concessionaires','construction storage',\n",
    "                      'consumer electronics','copy centers','copying  duplicating services','corolla ','cosmetics','crab house ','craft supplies','craig ', \n",
    "                      'creole  cajun ', 'creole cajun ' ,'crestview acres','crooksville ' ,'crosstown plaza','crystal river ','cumberland furnace ','cuyahoga falls ' \n",
    "                      ,'dayton','dayton','delivery','deordorant','delicatessen','delicatessen', 'des moines ','dialysis','dialysis clinics','diesel fuel','discover',\n",
    "                      'dive ','doctor','dog run','dothan ','dvd','e commerce','e-commerce','eastern european ', 'eating','educational materials',\n",
    "                      'educational service-business','electric contractors','electronic publishing','employment opportunities','english ' ,\n",
    "                      'engravers','entertainment  arts','essential oils','ethnic food markets','ethnic markets','european','event planners','event planning',\n",
    "                      'event ticket agencies','evergreen ' ,'exporters','face cleansers','family ' ,'family entertainment','family-friendly dining','farming service',\n",
    "                      'farmington ' ,'farms','filipino','finance  financial services','financial planning','fine dining ','fish  seafood markets','fish  seafood retail',\n",
    "                      'fish market','fishing tackle','florists','food  beverage s','food  dining','food  entertainment','food court','food dining','food drink',\n",
    "                      'food east columbus','food products','food s','food service','food service management','food truck','foods','forever living','fort pierce ',\n",
    "                      'franchising','fund raising games','fur repair','furniture','fusion','fusion ','gadsden ','gainesville ', 'garden centers','garment services',\n",
    "                      'gas station','gay lesbian ','general contractors','general entertainment','general merchandise-retail','geologists','german ','gillette',\n",
    "                      'gladstone ', 'gluten-free','gluten-free','global','glue','gluten-free foods','golf courses','golf practice ranges','gourmet ', 'grand island ',\n",
    "                      'grants pass ','greek  park 100','green products','grocers-ethnic foods','hair care products','hand sanitizer','hawaiian', 'headquarters',\n",
    "                      'health care providers','health clinics', 'health food ','health food store','health medical services','healthy','hermiston ', 'hilo ',\n",
    "                      'historic sites','historic walking areas','holding companies','home furnishings','home services  furnishings','hospitals','hotel bar',\n",
    "                      'hunan ', 'huntersville','huntersville', 'huntington ', ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5036317f",
   "metadata": {
    "papermill": {
     "duration": 0.525586,
     "end_time": "2024-11-01T03:59:14.061374",
     "exception": false,
     "start_time": "2024-11-01T03:59:13.535788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3153cf",
   "metadata": {
    "papermill": {
     "duration": 0.449842,
     "end_time": "2024-11-01T03:59:14.970059",
     "exception": false,
     "start_time": "2024-11-01T03:59:14.520217",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "941e826b",
   "metadata": {
    "papermill": {
     "duration": 0.444891,
     "end_time": "2024-11-01T03:59:15.863789",
     "exception": false,
     "start_time": "2024-11-01T03:59:15.418898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Reduced Columns 1491 to 1249"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd277875",
   "metadata": {
    "papermill": {
     "duration": 0.521565,
     "end_time": "2024-11-01T03:59:16.835727",
     "exception": false,
     "start_time": "2024-11-01T03:59:16.314162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bar_plot = pd.DataFrame()\n",
    "bar_plot['cat'] = new_df1.columns[1:]\n",
    "bar_plot['count'] = new_df1.iloc[:,1:].sum().values\n",
    "bar_plot.sort_values(['count'], inplace=True, ascending=False)\n",
    "bar_plot.reset_index(inplace=True, drop=True)\n",
    "bar_plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73830dbc",
   "metadata": {
    "papermill": {
     "duration": 2.792782,
     "end_time": "2024-11-01T03:59:20.085330",
     "exception": false,
     "start_time": "2024-11-01T03:59:17.292548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#taking the cusines that count more than 50\n",
    "main_categories = bar_plot[bar_plot['count']>50]\n",
    "categories = main_categories['cat'].values\n",
    "categories = np.append(categories,'Others')\n",
    "not_category = []\n",
    "new_df1['Others'] = 0\n",
    "\n",
    "for i in new_df1.columns[1:]:\n",
    "    if i not in categories:\n",
    "        new_df1.loc[new_df1[i] == 1, 'Others'] = 1\n",
    "        not_category.append(i)\n",
    "\n",
    "new_df1.drop(not_category, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a43eb",
   "metadata": {
    "papermill": {
     "duration": 0.475245,
     "end_time": "2024-11-01T03:59:21.014796",
     "exception": false,
     "start_time": "2024-11-01T03:59:20.539551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df1 = new_df1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43688068",
   "metadata": {
    "papermill": {
     "duration": 0.4655,
     "end_time": "2024-11-01T03:59:21.926958",
     "exception": false,
     "start_time": "2024-11-01T03:59:21.461458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e41e2",
   "metadata": {
    "papermill": {
     "duration": 0.470342,
     "end_time": "2024-11-01T03:59:22.845203",
     "exception": false,
     "start_time": "2024-11-01T03:59:22.374861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bar_plot1 = pd.DataFrame()\n",
    "bar_plot1['cat'] = new_df1.columns[1:]\n",
    "categories1= bar_plot1['cat'].values\n",
    "categories1 = np.append(categories1,'Others')\n",
    "categories1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1fc483",
   "metadata": {
    "papermill": {
     "duration": 0.471948,
     "end_time": "2024-11-01T03:59:23.764433",
     "exception": false,
     "start_time": "2024-11-01T03:59:23.292485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "most_common_cat = pd.DataFrame()\n",
    "most_common_cat['cat'] = new_df1.columns[1:]\n",
    "most_common_cat['count'] = new_df1.iloc[:,1:].sum().values\n",
    "most_common_cat.sort_values(['count'], inplace=True, ascending=False)\n",
    "most_common_cat.reset_index(inplace=True, drop=True)\n",
    "most_common_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10780ed",
   "metadata": {
    "papermill": {
     "duration": 3.76325,
     "end_time": "2024-11-01T03:59:27.975695",
     "exception": false,
     "start_time": "2024-11-01T03:59:24.212445",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming `most_common_cat` is defined as your DataFrame containing 'cat' and 'count' columns\n",
    "plt.figure(figsize=(25, 8))\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "pal = sns.color_palette(\"Blues_r\", len(most_common_cat))\n",
    "rank = most_common_cat['count'].argsort().argsort()\n",
    "\n",
    "# Corrected line with x, y, and data passed as named arguments\n",
    "sns.barplot(x='cat', y='count', data=most_common_cat, palette=np.array(pal[::-1])[rank])\n",
    "plt.axhline(ls='--', c='red')\n",
    "plt.title(\"Most commons categories\", fontsize=24)\n",
    "plt.ylabel('Number of titles', fontsize=18)\n",
    "plt.xlabel('Genre', fontsize=18)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439516e7",
   "metadata": {
    "papermill": {
     "duration": 0.458464,
     "end_time": "2024-11-01T03:59:28.907726",
     "exception": false,
     "start_time": "2024-11-01T03:59:28.449262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new_df1 = new_df1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2286de2",
   "metadata": {
    "papermill": {
     "duration": 0.484676,
     "end_time": "2024-11-01T03:59:29.845764",
     "exception": false,
     "start_time": "2024-11-01T03:59:29.361088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rowSums = new_df1.iloc[:,1:].sum(axis=1)\n",
    "multiLabel_counts = rowSums.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4923b4",
   "metadata": {
    "papermill": {
     "duration": 0.852503,
     "end_time": "2024-11-01T03:59:31.149218",
     "exception": false,
     "start_time": "2024-11-01T03:59:30.296715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "sns.set_style('whitegrid') \n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Corrected line: Use named arguments for x and y\n",
    "sns.barplot(x=multiLabel_counts.index, y=multiLabel_counts.values)\n",
    "plt.title(\"Number of categories per title\", fontsize=24)\n",
    "plt.ylabel('Number of titles', fontsize=18)\n",
    "plt.xlabel('Number of categories', fontsize=18)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d725b22",
   "metadata": {
    "papermill": {
     "duration": 0.547931,
     "end_time": "2024-11-01T03:59:32.150239",
     "exception": false,
     "start_time": "2024-11-01T03:59:31.602308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "boxplot = new_df1.copy()\n",
    "boxplot['len'] = new_df1.detailed_address.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab38a6cc",
   "metadata": {
    "papermill": {
     "duration": 0.769429,
     "end_time": "2024-11-01T03:59:33.393431",
     "exception": false,
     "start_time": "2024-11-01T03:59:32.624002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\",rc={\"font.size\":13,\"axes.labelsize\":13})\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "\n",
    "ax = sns.boxplot(x='len', data=boxplot, orient=\"h\", palette=\"Set2\")\n",
    "plt.ylabel('')\n",
    "plt.xlabel('Words')\n",
    "plt.title(\"Distribution of the word frequency\", fontsize=13)\n",
    "plt.tight_layout(h_pad=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801e9a3f",
   "metadata": {
    "papermill": {
     "duration": 10.138618,
     "end_time": "2024-11-01T03:59:43.984982",
     "exception": false,
     "start_time": "2024-11-01T03:59:33.846364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud,STOPWORDS\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "text = new_df1.detailed_address.values\n",
    "cloud = WordCloud(\n",
    "                          stopwords=STOPWORDS,\n",
    "                          background_color='black',\n",
    "                          collocations=False,\n",
    "                          width=2500,\n",
    "                          height=1800\n",
    "                         ).generate(\" \".join(text))\n",
    "plt.axis('off')\n",
    "plt.title(\"Common words on the description\",fontsize=40)\n",
    "plt.imshow(cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bae0f1",
   "metadata": {
    "papermill": {
     "duration": 0.477995,
     "end_time": "2024-11-01T03:59:44.947887",
     "exception": false,
     "start_time": "2024-11-01T03:59:44.469892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Making columns for result** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ad9bee",
   "metadata": {
    "papermill": {
     "duration": 0.494174,
     "end_time": "2024-11-01T03:59:45.921575",
     "exception": false,
     "start_time": "2024-11-01T03:59:45.427401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame to store results for each model across different metrics\n",
    "results = pd.DataFrame(columns=['Model', 'AUC', 'Precision', 'Recall', 'F1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf9c0c",
   "metadata": {
    "papermill": {
     "duration": 0.489821,
     "end_time": "2024-11-01T03:59:46.891376",
     "exception": false,
     "start_time": "2024-11-01T03:59:46.401555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "seeds = [1, 43, 678, 90, 135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e5b5a",
   "metadata": {
    "papermill": {
     "duration": 0.492568,
     "end_time": "2024-11-01T03:59:47.871617",
     "exception": false,
     "start_time": "2024-11-01T03:59:47.379049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = results.copy()\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ed6209",
   "metadata": {
    "papermill": {
     "duration": 0.483094,
     "end_time": "2024-11-01T03:59:48.842578",
     "exception": false,
     "start_time": "2024-11-01T03:59:48.359484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **TfidfVectorizer** and train-test spilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f309b5",
   "metadata": {
    "papermill": {
     "duration": 2.01728,
     "end_time": "2024-11-01T03:59:51.346518",
     "exception": false,
     "start_time": "2024-11-01T03:59:49.329238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(new_df1['detailed_address'], \n",
    "                                                    new_df1[new_df1.columns[1:]], \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=seeds[4], \n",
    "                                                    shuffle=True)\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4b2ca7",
   "metadata": {
    "papermill": {
     "duration": 0.484512,
     "end_time": "2024-11-01T03:59:52.317831",
     "exception": false,
     "start_time": "2024-11-01T03:59:51.833319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee895a98-ce27-49a1-ae12-af8b7b93d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this function before the evaluation loops\n",
    "def train_and_evaluate_model(model_pipeline, model_name, X_train, y_train, X_test, y_test):\n",
    "    auc = precision = recall = f1 = 0\n",
    "    total_categories = len(y_train.columns)\n",
    "\n",
    "    for category in y_train.columns:\n",
    "        print(f'**Processing {category} titles...**')\n",
    "\n",
    "        model_pipeline.fit(X_train, y_train[category])\n",
    "        prediction = model_pipeline.predict(X_test)\n",
    "\n",
    "        auc += roc_auc_score(y_test[category], prediction)\n",
    "        precision += precision_score(y_test[category], prediction, zero_division=1)\n",
    "        recall += recall_score(y_test[category], prediction, zero_division=1)\n",
    "        f1 += f1_score(y_test[category], prediction, zero_division=1)\n",
    "\n",
    "    average_auc = auc / total_categories\n",
    "    average_precision = precision / total_categories\n",
    "    average_recall = recall / total_categories\n",
    "    average_f1 = f1 / total_categories\n",
    "\n",
    "    print(f'Average AUC ROC is {average_auc}')\n",
    "    print(f'Average Precision is {average_precision}')\n",
    "    print(f'Average Recall is {average_recall}')\n",
    "    print(f'Average F1 Score is {average_f1}')\n",
    "\n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'AUC': average_auc,\n",
    "        'Precision': average_precision,\n",
    "        'Recall': average_recall,\n",
    "        'F1': average_f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecd67db",
   "metadata": {
    "papermill": {
     "duration": 35.574847,
     "end_time": "2024-11-01T04:00:28.371422",
     "exception": false,
     "start_time": "2024-11-01T03:59:52.796575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "LR_pipeline = Pipeline([('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1))])\n",
    "results = results.append(train_and_evaluate_model(LR_pipeline, 'Logistic Regression (OneVsAll)', X_train, y_train, X_test, y_test), ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1722705",
   "metadata": {
    "papermill": {
     "duration": 2.330645,
     "end_time": "2024-11-01T04:00:31.202519",
     "exception": false,
     "start_time": "2024-11-01T04:00:28.871874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Naive Bayes\n",
    "NB_pipeline = Pipeline([('clf', OneVsRestClassifier(MultinomialNB(fit_prior=True, class_prior=None)))])\n",
    "results = results.append(train_and_evaluate_model(NB_pipeline, 'Naive Bayes', X_train, y_train, X_test, y_test), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5afb37",
   "metadata": {
    "papermill": {
     "duration": 4.874073,
     "end_time": "2024-11-01T04:00:36.573425",
     "exception": false,
     "start_time": "2024-11-01T04:00:31.699352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# SVM\n",
    "SVC_pipeline = Pipeline([('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1))])\n",
    "results = results.append(train_and_evaluate_model(SVC_pipeline, 'SVM', X_train, y_train, X_test, y_test), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec5eb6a",
   "metadata": {
    "papermill": {
     "duration": 379.102248,
     "end_time": "2024-11-01T04:06:56.178040",
     "exception": false,
     "start_time": "2024-11-01T04:00:37.075792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "RF_pipeline = Pipeline([('clf', OneVsRestClassifier(RandomForestClassifier(), n_jobs=-1))])\n",
    "results = results.append(train_and_evaluate_model(RF_pipeline, 'Random Forest', X_train, y_train, X_test, y_test), ignore_index=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9289dfa5",
   "metadata": {
    "papermill": {
     "duration": 0.518849,
     "end_time": "2024-11-01T04:06:57.198118",
     "exception": false,
     "start_time": "2024-11-01T04:06:56.679269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1229728f",
   "metadata": {
    "papermill": {
     "duration": 0.536303,
     "end_time": "2024-11-01T04:06:58.266347",
     "exception": false,
     "start_time": "2024-11-01T04:06:57.730044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#this will take so much time so run for only once and save the results\n",
    "'''from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = BinaryRelev(GausancesianNB())\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "accuracy_score(y_test,predictions)\n",
    "print('AUC ROC is {}'.format(roc_auc_score(y_test,predictions.toarray())))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5722cbe",
   "metadata": {
    "papermill": {
     "duration": 0.507298,
     "end_time": "2024-11-01T04:06:59.276318",
     "exception": false,
     "start_time": "2024-11-01T04:06:58.769020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#results.loc[4,'BinaryRelevance'] = roc_auc_score(y_test,predictions.toarray())\n",
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4dfb1f",
   "metadata": {
    "papermill": {
     "duration": 0.516549,
     "end_time": "2024-11-01T04:07:00.309477",
     "exception": false,
     "start_time": "2024-11-01T04:06:59.792928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#this will take so much time so run for only once and save the results\n",
    "'''from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = ClassifierChain(LogisticRegression())\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "print('AUC ROC is {}'.format(roc_auc_score(y_test,predictions.toarray())))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6283c7",
   "metadata": {
    "papermill": {
     "duration": 0.506267,
     "end_time": "2024-11-01T04:07:01.313116",
     "exception": false,
     "start_time": "2024-11-01T04:07:00.806849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#results.loc[4,'ClassifierChain'] = roc_auc_score(y_test,predictions.toarray())\n",
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0063a9",
   "metadata": {
    "papermill": {
     "duration": 121.076081,
     "end_time": "2024-11-01T04:09:02.892530",
     "exception": false,
     "start_time": "2024-11-01T04:07:01.816449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = MultiOutputClassifier(KNeighborsClassifier()).fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "auc = roc_auc_score(y_test, predictions, average='macro')\n",
    "precision = precision_score(y_test, predictions, average='macro', zero_division=1)\n",
    "recall = recall_score(y_test, predictions, average='macro', zero_division=1)\n",
    "f1 = f1_score(y_test, predictions, average='macro', zero_division=1)\n",
    "\n",
    "print(f'AUC ROC: {auc}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b446eae1",
   "metadata": {
    "papermill": {
     "duration": 0.684886,
     "end_time": "2024-11-01T04:09:04.074565",
     "exception": false,
     "start_time": "2024-11-01T04:09:03.389679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = results.append({\n",
    "    'Model': 'MultipleOutput (KNN)',\n",
    "    'AUC': auc,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1': f1\n",
    "}, ignore_index=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4d1841",
   "metadata": {
    "papermill": {
     "duration": 0.539396,
     "end_time": "2024-11-01T04:09:05.105601",
     "exception": false,
     "start_time": "2024-11-01T04:09:04.566205",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Deep Learning**\n",
    "Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2dd03c-50fd-46b6-bcf8-e0a2581c4408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate deep learning model predictions\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    # Convert predictions to binary values (threshold at 0.5)\n",
    "    predictions_binary = (predictions > 0.5).astype(int)\n",
    "\n",
    "    auc = roc_auc_score(y_test, predictions_binary, average='macro')\n",
    "    precision = precision_score(y_test, predictions_binary, average='macro', zero_division=1)\n",
    "    recall = recall_score(y_test, predictions_binary, average='macro', zero_division=1)\n",
    "    f1 = f1_score(y_test, predictions_binary, average='macro', zero_division=1)\n",
    "\n",
    "    print(f'AUC ROC: {auc}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "\n",
    "    return auc, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc136470",
   "metadata": {
    "papermill": {
     "duration": 0.51209,
     "end_time": "2024-11-01T04:09:06.138210",
     "exception": false,
     "start_time": "2024-11-01T04:09:05.626120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df1['detailed_address']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e84b0",
   "metadata": {
    "papermill": {
     "duration": 8.750458,
     "end_time": "2024-11-01T04:09:15.383631",
     "exception": false,
     "start_time": "2024-11-01T04:09:06.633173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000, lower=True)\n",
    "tokenizer.fit_on_texts(new_df1['detailed_address'])\n",
    "sequences = tokenizer.texts_to_sequences(new_df1['detailed_address'])\n",
    "x = pad_sequences(sequences, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767916ee",
   "metadata": {
    "papermill": {
     "duration": 0.527767,
     "end_time": "2024-11-01T04:09:16.412040",
     "exception": false,
     "start_time": "2024-11-01T04:09:15.884273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, \n",
    "                                                    new_df1[new_df1.columns[1:]], \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=seeds[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1406c5d0",
   "metadata": {
    "papermill": {
     "duration": 0.575156,
     "end_time": "2024-11-01T04:09:18.488166",
     "exception": false,
     "start_time": "2024-11-01T04:09:17.913010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove duplicate class_weight calculation and generalize\n",
    "most_common_cat['class_weight'] = len(most_common_cat) / most_common_cat['count']\n",
    "\n",
    "# Generalized class weight calculation by converting it to a dictionary\n",
    "class_weight = most_common_cat.set_index('cat')['class_weight'].to_dict()\n",
    "\n",
    "\n",
    "# Display the updated DataFrame\n",
    "most_common_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345af2e",
   "metadata": {
    "papermill": {
     "duration": 0.511808,
     "end_time": "2024-11-01T04:09:19.492716",
     "exception": false,
     "start_time": "2024-11-01T04:09:18.980908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = y_train.shape[1]\n",
    "max_words = len(tokenizer.word_index) + 1\n",
    "maxlen = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c69ad30",
   "metadata": {
    "papermill": {
     "duration": 0.512361,
     "end_time": "2024-11-01T04:09:21.529768",
     "exception": false,
     "start_time": "2024-11-01T04:09:21.017407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **DNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6106d62f",
   "metadata": {
    "papermill": {
     "duration": 0.92186,
     "end_time": "2024-11-01T04:09:22.952908",
     "exception": false,
     "start_time": "2024-11-01T04:09:22.031048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam # - Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f7848f",
   "metadata": {
    "papermill": {
     "duration": 44.491544,
     "end_time": "2024-11-01T04:10:08.001092",
     "exception": false,
     "start_time": "2024-11-01T04:09:23.509548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalMaxPool1D, Dropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Updated Embedding layer (Removed input_length argument as it is deprecated)\n",
    "model.add(Embedding(input_dim=max_words, output_dim=20))\n",
    "\n",
    "# Adding a dropout layer (uncomment if you need it)\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# GlobalMaxPool1D and final Dense layer\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=[tf.keras.metrics.AUC(), Precision(), Recall()])\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(),\n",
    "    # Uncomment if you need early stopping\n",
    "    # EarlyStopping(patience=10),\n",
    "    ModelCheckpoint(filepath='model-simple.keras', save_best_only=True)  # Use `.keras` file extension as per the new standard\n",
    "]\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    class_weight=class_weight,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_split=0.3,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0789f8",
   "metadata": {
    "papermill": {
     "duration": 1.323401,
     "end_time": "2024-11-01T04:10:09.893472",
     "exception": false,
     "start_time": "2024-11-01T04:10:08.570071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dnn_model = model\n",
    "# Metrics evaluation for DNN Model\n",
    "metrics = evaluate_model(dnn_model, X_test, y_test)\n",
    "results = results.append({\n",
    "    'Model': 'DNN',\n",
    "    'AUC': metrics[0],\n",
    "    'Precision': metrics[1],\n",
    "    'Recall': metrics[2],\n",
    "    'F1': metrics[3]\n",
    "}, ignore_index=True)\n",
    "\n",
    "# Similarly, use the same function to evaluate CNN, LSTM, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea9eab",
   "metadata": {
    "papermill": {
     "duration": 0.544907,
     "end_time": "2024-11-01T04:10:12.114045",
     "exception": false,
     "start_time": "2024-11-01T04:10:11.569138",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee5827",
   "metadata": {
    "papermill": {
     "duration": 133.653101,
     "end_time": "2024-11-01T04:12:26.312086",
     "exception": false,
     "start_time": "2024-11-01T04:10:12.658985",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "filter_length = 300\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Updated Embedding layer (Removed input_length argument)\n",
    "model.add(Embedding(input_dim=max_words, output_dim=20))\n",
    "\n",
    "# Optionally add dropout\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# Add 1D Convolution layer\n",
    "model.add(Conv1D(filters=filter_length, kernel_size=3, padding='valid', activation='relu', strides=1))\n",
    "\n",
    "# Add pooling and output layers\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=[tf.keras.metrics.AUC(), Precision(), Recall()])\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(),\n",
    "    ModelCheckpoint(filepath='model-conv1d.keras', save_best_only=True)  # Updated to use `.keras` extension\n",
    "]\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    class_weight=class_weight,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_split=0.3,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c93608",
   "metadata": {
    "papermill": {
     "duration": 1.628056,
     "end_time": "2024-11-01T04:12:28.632404",
     "exception": false,
     "start_time": "2024-11-01T04:12:27.004348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_model = model\n",
    "# Metrics evaluation for CNN Model\n",
    "metrics = evaluate_model(cnn_model, X_test, y_test)\n",
    "results = results.append({\n",
    "    'Model': 'CNN',\n",
    "    'AUC': metrics[0],\n",
    "    'Precision': metrics[1],\n",
    "    'Recall': metrics[2],\n",
    "    'F1': metrics[3]\n",
    "}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba44937",
   "metadata": {
    "papermill": {
     "duration": 0.694357,
     "end_time": "2024-11-01T04:12:31.439369",
     "exception": false,
     "start_time": "2024-11-01T04:12:30.745012",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32a1eab6",
   "metadata": {
    "papermill": {
     "duration": 0.694032,
     "end_time": "2024-11-01T04:12:32.866053",
     "exception": false,
     "start_time": "2024-11-01T04:12:32.172021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **GloVe - LSTM** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f151562",
   "metadata": {
    "papermill": {
     "duration": 12.159732,
     "end_time": "2024-11-01T04:12:45.713700",
     "exception": false,
     "start_time": "2024-11-01T04:12:33.553968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((max_words, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf67f9",
   "metadata": {
    "papermill": {
     "duration": 946.903627,
     "end_time": "2024-11-01T04:28:33.312977",
     "exception": false,
     "start_time": "2024-11-01T04:12:46.409350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Flatten, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "# Input layer\n",
    "deep_inputs = Input(shape=(maxlen,))\n",
    "\n",
    "# Embedding layer using pre-trained weights (trainable=False)\n",
    "embedding_layer = Embedding(input_dim=max_words, output_dim=100, weights=[embedding_matrix], trainable=False)(deep_inputs)\n",
    "\n",
    "# LSTM layer\n",
    "LSTM_Layer_1 = LSTM(128)(embedding_layer)\n",
    "\n",
    "# Dense output layer\n",
    "dense_layer_1 = Dense(99, activation='sigmoid')(LSTM_Layer_1)\n",
    "\n",
    "# Creating the model\n",
    "model = Model(inputs=deep_inputs, outputs=dense_layer_1)\n",
    "\n",
    "# Callbacks - Updated ModelCheckpoint to use `.keras` extension\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(),\n",
    "    ModelCheckpoint(filepath='model-lstm.keras', save_best_only=True)  # Updated the file extension to `.keras`\n",
    "]\n",
    "\n",
    "# Compiling the model\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=[tf.keras.metrics.AUC(), Precision(), Recall()])\n",
    "\n",
    "\n",
    "# Fitting the model\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train.values,\n",
    "    class_weight=class_weight,\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    validation_split=0.3,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193ac454",
   "metadata": {
    "papermill": {
     "duration": 7.261636,
     "end_time": "2024-11-01T04:28:41.627756",
     "exception": false,
     "start_time": "2024-11-01T04:28:34.366120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Metrics evaluation for LSTM Model\n",
    "lstm_model = model\n",
    "metrics = evaluate_model(lstm_model, X_test, y_test)\n",
    "results = results.append({\n",
    "    'Model': 'LSTM',\n",
    "    'AUC': metrics[0],\n",
    "    'Precision': metrics[1],\n",
    "    'Recall': metrics[2],\n",
    "    'F1': metrics[3]\n",
    "}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7945768b",
   "metadata": {
    "papermill": {
     "duration": 0.98074,
     "end_time": "2024-11-01T04:28:45.663195",
     "exception": false,
     "start_time": "2024-11-01T04:28:44.682455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1599bcb7",
   "metadata": {
    "papermill": {
     "duration": 1.071367,
     "end_time": "2024-11-01T04:28:47.790914",
     "exception": false,
     "start_time": "2024-11-01T04:28:46.719547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Feature Scaling because yes we don't want one independent variable dominating the other and it makes computations easy\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4ad708",
   "metadata": {
    "papermill": {
     "duration": 1.096704,
     "end_time": "2024-11-01T04:28:49.881733",
     "exception": false,
     "start_time": "2024-11-01T04:28:48.785029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sequential model to initialise our ann and dense module to build the layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "#Initialising ANN\n",
    "ann = tf.keras.models.Sequential()\n",
    " #Adding First Hidden Layer\n",
    "ann.add(tf.keras.layers.Dense(units=1,activation=\"relu\"))\n",
    " #Adding Second Hidden Layer\n",
    "ann.add(tf.keras.layers.Dense(units=6,activation=\"relu\"))\n",
    "#Adding Output Layer\n",
    "ann.add(tf.keras.layers.Dense(units=num_classes,activation=\"sigmoid\"))\n",
    "#Compiling ANN\n",
    "\n",
    "ann.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=[tf.keras.metrics.AUC(), Precision(), Recall()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f01ab4",
   "metadata": {
    "papermill": {
     "duration": 1.044329,
     "end_time": "2024-11-01T04:28:51.919556",
     "exception": false,
     "start_time": "2024-11-01T04:28:50.875227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fitting ANN\n",
    "ann.fit(X_train, y_train, batch_size=30, epochs=10, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6292d-f1e5-4593-b711-17091139de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics evaluation for ANN Model\n",
    "metrics = evaluate_model(ann, X_test, y_test)  # Assuming `ann` is your ANN instance\n",
    "results = results.append({\n",
    "    'Model': 'ANN',\n",
    "    'AUC': metrics[0],\n",
    "    'Precision': metrics[1],\n",
    "    'Recall': metrics[2],\n",
    "    'F1': metrics[3]\n",
    "}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79d909e",
   "metadata": {
    "papermill": {
     "duration": 0.995623,
     "end_time": "2024-11-01T04:28:53.978230",
     "exception": false,
     "start_time": "2024-11-01T04:28:52.982607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7572bab9-b24e-4ea8-8eb3-9efab7f1a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_min(s, color='red'):\n",
    "    is_min = s == s.min()\n",
    "    return ['background-color: {}'.format(color) if v else '' for v in is_min]\n",
    "\n",
    "def highlight_max(s, color='lightgreen'):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: {}'.format(color) if v else '' for v in is_max]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2b06f",
   "metadata": {
    "papermill": {
     "duration": 1.114261,
     "end_time": "2024-11-01T04:29:00.175233",
     "exception": false,
     "start_time": "2024-11-01T04:28:59.060972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.style.apply(highlight_min, color='red', axis=None).apply(highlight_max, color='lightgreen', axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51dc8a0",
   "metadata": {
    "papermill": {
     "duration": 1.038284,
     "end_time": "2024-11-01T04:29:02.206860",
     "exception": false,
     "start_time": "2024-11-01T04:29:01.168576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "#cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "cm = LinearSegmentedColormap.from_list(\n",
    "    name='test', \n",
    "    #colors=['red','white','green','white','red']\n",
    "    colors=['tomato','orange','white','lightgreen','green']\n",
    ")\n",
    "\n",
    "t = results.apply(pd.to_numeric).style.background_gradient(cmap=cm)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb827f5d-d2ad-4b03-8dcd-b03333500e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results dictionary to DataFrame\n",
    "metrics_data = []\n",
    "\n",
    "# Extract the data from the results DataFrame\n",
    "for index, row in results.iterrows():\n",
    "    metrics_data.append([row['Model'], row['AUC'], row['Precision'], row['Recall'], row['F1']])\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data, columns=['Model', 'AUC', 'Precision', 'Recall', 'F1'])\n",
    "\n",
    "# Plot the results using bar plots for each metric\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "\n",
    "# Plot AUC\n",
    "sns.barplot(data=metrics_df, x='Model', y='AUC', ax=axes[0, 0], palette='viridis')\n",
    "axes[0, 0].set_title('AUC Comparison')\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "\n",
    "# Plot Precision\n",
    "sns.barplot(data=metrics_df, x='Model', y='Precision', ax=axes[0, 1], palette='plasma')\n",
    "axes[0, 1].set_title('Precision Comparison')\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "\n",
    "# Plot Recall\n",
    "sns.barplot(data=metrics_df, x='Model', y='Recall', ax=axes[1, 0], palette='inferno')\n",
    "axes[1, 0].set_title('Recall Comparison')\n",
    "axes[1, 0].set_ylim(0, 1)\n",
    "\n",
    "# Plot F1 Score\n",
    "sns.barplot(data=metrics_df, x='Model', y='F1', ax=axes[1, 1], palette='magma')\n",
    "axes[1, 1].set_title('F1 Score Comparison')\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "\n",
    "# Set common formatting for all plots\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8968731",
   "metadata": {
    "papermill": {
     "duration": 0.990443,
     "end_time": "2024-11-01T04:29:04.246321",
     "exception": false,
     "start_time": "2024-11-01T04:29:03.255878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "def extract_code_from_ipynb(ipynb_file, output_file):\n",
    "    with open(ipynb_file, 'r', encoding='utf-8') as file:\n",
    "        notebook = nbformat.read(file, as_version=4)\n",
    "        \n",
    "    # Extract only code cells\n",
    "    code_cells = [cell['source'] for cell in notebook['cells'] if cell['cell_type'] == 'code']\n",
    "    \n",
    "    # Write each code cell into output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for code in code_cells:\n",
    "            file.write(code)\n",
    "            file.write('\\n\\n')\n",
    "\n",
    "# Example use case to extract code into a .py file\n",
    "extract_code_from_ipynb('multilabel-classification-cnn-dnn-lstm.ipynb', 'dl.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8980a1-5d00-4984-a7d7-fee828792303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1857.250924,
   "end_time": "2024-11-01T04:29:09.886035",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-01T03:58:12.635111",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
